Skeleton 3D Electrostatic MPI/OpenMP Particle-in-Cell (PIC) codes
by Viktor K. Decyk
copyright 2000-2015, regents of the university of california

This program contains sample codes for illustrating the basic structure
of a 3D Electrostatic MPI/OpenMP Particle-in-Cell (PIC) code, in both
Fortran and C.  The codes have no diagnosics except for initial and
final energies.  Their primary purpose is to provide example codes for
physical science students learning about MPI/OpenMP PIC codes.  They are
also intended as benchmark reference codes to aid in developing new
codes and in evaluating new computer architectures.  Parallel versions
of this code with the same structure for MPI only (ppic3) and OpenMP
only (mpic3) also exist, and can be compared to this code in order to
understand the parallel algorithms.

PIC codes are widely used in plasma physics.  They model plasmas as
particles which interact self-consistently via the electromagnetic
fields they themselves produce.  PIC codes generally have three
important procedures in the main iteration loop.  The first is the
deposit, where some particle quantity, such as a charge, is accumulated
on a grid via interpolation to produce a source density.  The second
important procedure is the field solver, which solves Maxwell’s equation
or a subset to obtain the electric and/or magnetic fields from the
source densities.  Finally, once the fields are obtained, the particle
forces are found by interpolation from the grid, and the particle
co-ordinates are updated, using Newton’s second law and the Lorentz
force.  The particle processing parts dominate over the field solving
parts in a typical PIC application. 

More details about PIC codes can be found in the texts by C. K. Birdsall
and A. B. Langdon, Plasma Physics via Computer Simulation, 1985,
R. W. Hockney and J. W. Eastwood, Computer Simulation Using Particles,
1981, and John M. Dawson, "Particle simulation of plasmas", Rev. Mod.
Phys. 55, 403 (1983).  Details about the mathematical equations and
units used in this code is given in the companion article,
"Description of Electrostatic Spectral Code from the UPIC Framework" by
Viktor K. Decyk, UCLA, in the file ESModels.pdf.

Details abut MPI can be found in the book by William Gropp, Ewing Lusk,
and Anthony Skjellum, Using MPI: Portable Parallel Programming with the
Message-Passing Interface, The MIT Press, 1994.  Details abut OpenMP can
be found in the book by Rohit Chandra, Leonardo Dagum, Dave Kohr, 
Dror Maydan, Jeff McDonald, and Ramesh Menon, Parallel Programming in
OpenMP, Morgan Kaufmann, 2001.

No warranty for proper operation of this software is given or implied.
Software or information may be copied, distributed, and used at own
risk; it may not be distributed without this notice included verbatim
with each file.  If use of these codes results in a publication, an
acknowledgement is requested.

The code here uses the simplest force, the electrostatic Coulomb
interaction, obtained by solving a Poisson equation.  A spectral method
using Fast Fourier Transforms (FFTs) is used to solve the Poisson
equation.  A real to complex FFT is used, and the data in Fourier space
is stored in a packed format, where the input and output sizes are the
same.  The boundary conditions are periodic, only electron species are
included, and linear interpolation is used.

For parallelization, the code uses two levels of parallelism.  The
outermost level uses a simple 2D domain decomposition scheme, where the
field quantities (electric field, charge density) are divided among the
computational nodes.  The primary decomposition divides the y and z
values evenly, that is, each node has all the x values for some y and
some z.  The particles are distributed so that the y and z co-ordinates
of the particles have a value within the domain.  This simple
decomposition works if the particles are uniformly distributed in space.
Particles at the edges of the domain may need information from the
nearby domains in order to interpolate the fields.  To avoid unnecessary
communication, one extra guard cell in y and z is added at the end of
each domain that replicates the first y and z values in the nearby
domains.  After particles are updated, some particles may move to a
neighboring domain.  A particle manager (PPPMOVE32) is responsible for
moving such particles to the appropriate domain.  The FFT is performed
in 5 steps.  In going from real space to Fourier space, the FFT is first
performed in x for the y and z values in the primary domain.  The data
is then transposed to a secondary domain decomposition, where each node
has all the y values for some x and z.  The FFT is then performed in the
y direction for the x and z values in the secondary domain.  Finally,
the data is then transposed to a tertiary domain decomposition, where
each node has all the z values for some x and y.  The FFT is then
performed in the z direction for the x and y values in the tertiary
domain.  Poisson's equation is solved using this tertiary decomposition.
There are four main communication procedures which use MPI.  The first
adds the guard cells for the charge density, the second copies the guard
cells for the electric field.  The third is the particle manager, and
the fourth transposes the data between the primary, secondary, and
tertiary decompositions using a some to some communication pattern.
Further information about the domain decomposition parallel algorithms
used can be found in the companion presentation Dcomp.pdf and in the
article: p. c. liewer and v. k. decyk, j. computational phys. 85, 302
(1989).

On each MPI node, a second level of parallelism is used.  The innermost
level uses a tiling (or blocking) technique.  Space is divided into
small 3D tiles (with typically 8x8x8 grid points in a tile), and
particles are organized so that their co-ordinates in x, y, and z lie
within the same tile and are stored together in memory.  Assigning
different threads to different tiles avoids data collisions (where 2
threads try to write to the same memory location at the same time).  The
size of the tiles should be smaller than the L1 cache and the number of
tiles should be equal to or greater than the number of processing cores.
There are 3 major procedures which make use of the tiled data structure,
the charge deposit, the particle push, and the particle reordering.  In
the deposit procedure, each thread first deposits to a small local
density array the size of a tile plus guard cells.  After all the
particles in a tile have been processed the small density array is added
to the global charge density array.  The particle push is similar, where
each thread first copies the global field array to a small local field
array which is then used for field interpolation.  The particle
reordering step is divided into two procedures.  The first procedure
(PPORDERF32LA) moves particles which are leaving a tile into an ordered
particle buffer.  In addition, those particles which are leaving the MPI
node are copied to one of two MPI send buffers.  The MPI particle
manager (PPPMOVE32) then sends and receives the particle buffers.  The
second procedure (PPORDERF32LB) then copies the incoming particles,
either from the ordered particle buffer or from the MPI receive buffers,
and inserts the particles into the appropriate location in the particle
array.  Further information about this tiling parallel algorithm used
can be found in the article: V. K. Decyk and T. V. Singh,
"Particle-in-Cell Algorithms for Emerging Computer Architectures,"
Computer Physics Communications, 185, 708, 2014, available at
http://dx.doi.org/10.1016/j.cpc.2013.10.013.  Further information about
the hybrid domain decomposition/tiling scheme can be found in the
companion presentation MPI-OpenMP-PIC.pdf.

The default particle push calculates the list of particles leaving the
tiles.  This was done because the code was faster.  There is, however, a
version of the push (PPGPPUSH32L) which does not calculate the list, and
is easier to understand.  This version of the push requires a different
reordering procedure (PPPORDER32LA) which calculates the list.

Particles are initialized with a uniform distribution in space and a 
gaussian distribution in velocity space.  This describes a plasma in
thermal equilibrium.  The inner loop contains a charge deposit, add
guard cell procedures, a scalar FFT, a Poisson solver, a vector FFT,
copy guard cell procedures, a particle push, a particle manager, and a
particle sorting procedure.  The final energy and timings are printed.
A sample output file for the default input parameters is included in the
file output.

In more detail, the inner loop of the code contains the following
procedures in Fortran (C):

Deposit section:
   PPGPPOST32L (cppgppost32l): deposit charge density
   PPAGUARD32XL (cppaguard32xl): add charge density guard cells in x on
                                 local processor
   PPNAGUARD32L (cppnaguard32l): add charge density guard cells in y and
                                 z from remote processor

Field solve section:
   WPPFFT32RM (cwppfft32rm): FFT charge density to fourier space
   MPPOIS332 (cmppois332): calculate smoothed longitudinal electric
                           field in fourier space.
   WPPFFT32RM3 (cwppfft32rm3): FFT smoothed electric field to real space

Particle Push section:
   PPNCGUARD32L (cppncguard32l): fill in guard cells for smoothed
                                 electric field in y and z from remote
                                 processor
   PPCGUARD32XL (cppcguard32xl): fill in guard cells for smoothed
                                 electric field in x field on local
                                 processor
   PPGPPUSHF32L (cppgppushf32l): update particle co-ordinates with
                                 smoothed electric field. also calculate
                                 locations of particles leaving
                                 processor for PPPORDERF32LA.
                                 x(t)->x(t+dt); v(t-dt/2)->v(t+dt/2)
   PPPORDERF32LA (cppporderf32la): copies outgoing particles to buffers
                                   from list supplied by PPGPPUSHF32L.
   PPPMOVE32 (cpppmove32): moves particles to appropriate MPI node from
                           buffers supplied by PPPORDERF32LA.
   PPPORDER32LB (cppporder32lb): moves particles to appropriate tile.

The inputs to the code are the grid parameters indx, indy, indz, the
particle number parameters npx, npy, npz, the time parameters tend, dt,
the velocity parameters vtx, vty, vtz, vx0, vy0, vz0.  In addition, the
tile size mx, my, mz, and overflow size xtras are defined.

In more detail:
indx = exponent which determines length in x direction, nx=2**indx.
indy = exponent which determines length in y direction, ny=2**indy.
indz = exponent which determines length in z direction, nz=2**indz.
   These ensure the system lengths are a power of 2.
npx = number of electrons distributed in x direction.
npy = number of electrons distributed in y direction.
npz = number of electrons distributed in z direction.
   The total number of particles in the simulation is npx*npy*npz.
tend = time at end of simulation, in units of plasma frequency.
dt = time interval between successive calculations.
   The number of time steps the code runs is given by tend/dt.
   dt should be less than .2 for the electrostatic code.
vtx/vty/vtz = thermal velocity of electrons in x/y/z direction
   a typical value is 1.0.
vx0/vy0/vz0 = drift velocity of electrons in x/y/z direction.
mx/my/mz = number of grids points in x, y, and z in each tile
   should be less than or equal to 16.

The major program files contained here include:
mppic3.f90     Fortran90 main program 
mppic3.c       C main program
mpplib3.f      Fortran77 MPI communications library
mpplib3_h.f90  Fortran90 MPI communications interface (header) library
mpplib3.f90    Fortran90 MPI communications library
mpplib3.c      C MPI communications library [Not yet implemented]
mpplib3.h      C MPI communications header library
omplib.f       Fortran77 OpenMP utility library
omplib_h.f90   Fortran90 OpenMP utility interface (header) library
omplib.c       C OpenMP utility library
omplib.h       C OpenMP utility header library
mppush3.f      Fortran77 procedure library
mppush3_h.f90  Fortran90 procedure interface (header) library
mppush3.c      C procedure library [Not yet implemented]
mppush3.h      C procedure header library
dtimer.c       C timer function, used by both C and Fortran

Files with the suffix .f90 adhere to the Fortran 90 standard, files with
the suffix .f adhere to the Fortran77 standard, files with the suffix .c
and .h adhere to the C99 standard.

The makefile is setup to use gcc and gfortran with Linux.  A version for
Mac OS X is also in the Makefile, but is commented out.  

Two executables can be created, fmppic3 for Fortran, and cmppic3_f for a
C main program calling the Fortran procedure library.

To compile program, execute:

make program_name

where program_name is either: fmppic3 or cmppic3_f.

The command to execute a program with both MPI and OpenMP varies from
one system to another.  One possible command is:

mpiexec -np nproc -perhost n ./program_name

where program_name is either fmppic3 or cmppic3_f, and
where nproc is the number of processors to be used, and
where n is the number of MPI nodes per host.

There is one restriction on the number of processors which can be used:
this simple skeleton code does not support the case where MPI nodes have
zero grid points.  This special case can happen for certain combinations
of the grid size in y or z (set by the parameters indy or indz,
respectively) and the number of processors in y and z.  If this happens
the code will exit with an error message.  This special case will never
occur if the grid size in y is an exact multiple of the number of
processors in y, and the grid size in z is an exact multiple of the
number of processors in z.

By default, OpenMP will use the maximum number of processors it can find
on the MPI node.  If the user wants to control the number of threads, a
parameter nvpp can be set in the main program.  In addition, the
environment variable OMP_NUM_THREADS may need to be set to the maximum
number of threads per node expected.

The file output contains the results produced for the default parameters.

The Fortran version can be compiled to run with double precision by
changing the Makefile (typically by setting the compiler options flags
-r8).

The libraries mpplib3_f.c, omplib_f.c, and mppush3_f.c contain wrapper
functions to allow the Fortran libraries to be called from C.
