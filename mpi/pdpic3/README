Skeleton 3D Darwin MPI Particle-in-Cell (PIC) codes
by Viktor K. Decyk
copyright 2000-2015, regents of the university of california

This program contains sample codes for illustrating the basic structure
of a 3D Darwin MPI Particle-in-Cell (PIC) code, in both Fortran and C.
The codes have no diagnosics except for initial and final energies.
Their primary purpose is to provide example codes for physical science
students learning about MPI PIC codes.  They are also intended as
benchmark reference codes to aid in developing new codes and in
evaluating new computer architectures.  A serial version of this code
with the same structure (dpic3) also exists, and can be compared to this
code in order to understand the parallel algorithms.

PIC codes are widely used in plasma physics.  They model plasmas as
particles which interact self-consistently via the electromagnetic
fields they themselves produce.  PIC codes generally have three
important procedures in the main iteration loop.  The first is the
deposit, where some particle quantity, such as a charge, is accumulated
on a grid via interpolation to produce a source density.  The second
important procedure is the field solver, which solves Maxwell’s equation
or a subset to obtain the electric and/or magnetic fields from the
source densities.  Finally, once the fields are obtained, the particle
forces are found by interpolation from the grid, and the particle
co-ordinates are updated, using Newton’s second law and the Lorentz
force.  The particle processing parts dominate over the field solving
parts in a typical PIC application. 

More details about PIC codes can be found in the texts by C. K. Birdsall
and A. B. Langdon, Plasma Physics via Computer Simulation, 1985,
R. W. Hockney and J. W. Eastwood, Computer Simulation Using Particles,
1981, and John M. Dawson, "Particle simulation of plasmas", Rev. Mod.
Phys. 55, 403 (1983).  Details about the mathematical equations and
units used in this code is given in the companion article,
"Description of Darwin Spectral Code from the UPIC Framework,"
by Viktor K. Decyk, UCLA, in the file DModels.pdf.

Details abut MPI can be found in the book by William Gropp, Ewing Lusk,
and Anthony Skjellum, Using MPI: Portable Parallel Programming with the
Message-Passing Interface, The MIT Press, 1994.

No warranty for proper operation of this software is given or implied.
Software or information may be copied, distributed, and used at own
risk; it may not be distributed without this notice included verbatim
with each file.  If use of these codes results in a publication, an
acknowledgement is requested.

The code here uses the near-field electromagnetic interaction, obtained
by solving the Darwin subset of Maxwell's equation, where the transverse
displacement current in neglected in Ampere's law.  A spectral method
using Fast Fourier Transforms (FFTs) is used to solve the scalar and
vector Poisson equations.  A real to complex FFT is used, and the data
in Fourier space is stored in a packed format, where the input and
output sizes are the same.  The boundary conditions are periodic, only
electron species are included, and linear interpolation is used.
Additional details can be found in the companion presentation,
Darwin2.pdf.

For parallelization, the code uses a simple 2D domain decomposition
scheme, where the field quantities (electric and magnetic fields,
charge, current and current derivative densities) are divided among the
computational nodes.  The primary decomposition divides the y and z
values evenly, that is, each node has all the x values for some y and
some z.  The particles are distributed so that the y and z co-ordinates
of the particles have a value within the domain.  This simple
decomposition works if the particles are uniformly distributed in space.
Particles at the edges of the domain may need information from the
nearby domains in order to interpolate the fields.  To avoid unnecessary
communication, one extra guard cell in y and z is added at the end of
each domain that replicates the first y and z values in the nearby
domains.  After particles are updated, some particles may move to a
neighboring domain.  A particle manager (PPMOVE32) is responsible for
moving such particles to the appropriate domain.  The FFT is performed
in 5 steps.  In going from real space to Fourier space, the FFT is first
performed in x for the y and z values in the primary domain.  The data
is then transposed to a secondary domain decomposition, where each node
has all the y values for some x and z. The FFT is then performed in the
y direction for the x and z values in the secondary domain.  Finally,
the data is then transposed to a tertiary domain decomposition, where
each node has all the z values for some x and y.  The FFT is then
performed in the z direction for the x and y values in the tertiary
domain.  The Darwin equations are solved using this tertiary
decomposition.  There are five main communication procedures which use
MPI.  The first two add the guard cells for the charge, current and
current derivative densities, the third copies the guard
cells for the electric and magnetic fields.  The fourth is the particle
manager, and the fifth transposes the data between the primary,
secondary, and tertiary decompositions using a some to some
communication pattern.  Further information about the domain
decomposition parallel algorithms used can be found in the companion
presentation Dcomp.pdf and in the article: p. c. liewer and v. k. decyk,
j. computational phys. 85, 302 (1989).

Particles are initialized with a uniform distribution in space and a
gaussian distribution in velocity space.  This describes a plasma in
thermal equilibrium.  The inner loop contains a current, a current
derivative, and charge deposit, add guard cell procedures, a vector and
scalar FFT, transverse current procedures, Poisson solvers, vector FFTs,
copy guard cell procedures, a particle push, and a particle sorting
procedure.  The final energy and timings are printed.  A sample output
file for the default input parameters is included in the file output.

In more detail, the inner loop of the code contains the following
procedures in Fortran (C):

Deposit section:
   PPGJPOST32L (cppgjpost32l): deposit current density
   PPGPOST32L (cppgpost32l): deposit charge density
   PPGDJPOST32L (cppgdjpost32l): deposit acceleration density and
                                 momentum flux
   PPGDCJPOST32L (cppgdcjpost32l): deposit electron current and
                                   acceleration density and momentum
                                   flux
   PPASCFGUARD32L (cppascfguard32l): add acceleration density and scaled
                                     transverse electric field
   PPAGUARD32XL (cppaguard32xl): add charge density guard cells in x on
                                 local processor
   PPNAGUARD32L (cppnaguard32l): add charge density guard cells in y and
                                 z from remote processor 
   PPACGUARD32XL (cppacguard32xl): add current density guard cells in x
                                   on local processor
   PPNACGUARD32L (cppnacguard32l): add current density guard cells in y
                                   and z from remote processor
   PPACGUARD32XL (cppacguard32xl): add acceleration density guard cells
                                   in x on local processor
   PPNAGUARD32L (cppnacguard32l): add acceleration density guard cells
                                  in y and z from remote processor
   PPACGUARD32XL (cppacguard32xl): add momentum flux guard cells in x on
                                   local processor
   PPNAGUARD32L (cppnacguard32l): add momentum flux guard cells in y and
                                  z from remote processor

Field solve section:
   WPPFFT32R (cwppfft32r): FFT charge density to fourier space
   PPOIS332 (cppois332): calculate smoothed longitudinal electric field
                         in fourier space.
   WPPFFT32R3 (cwppfft32r3): FFT smoothed longitudinal electric field to
                             real space
   WPPFFT32R3 (cwppfft32r3): FFT current density to fourier space
   PPCUPERP32 (cppcuperp32): take transverse part of current
   PPBBPOISP332 (cppbbpoisp332): calculate magnetic field in fourier
                                 space
   WPPFFT32R3 (cwppfft32r3): FFT smoothed magnetic field to real space
   PPBADDEXT32 (cppbaddext32): add constant to magnetic field
   WPPFFT32R3 (cwppfft32r3): FFT acceleration density to fourier space
   WPPFFT32RN (cwppfft32rn): FFT momentum flux to fourier space
   PPADCUPERP32 (cppadcuperp32): take transverse part of time derivative
                                 of current from momentum flux and
                                 acceleration density
   PPEPOISP332(cppepoisp332): calculate transverse electric field
   WPPFFT32R3 (cwppfft32r3): FFT smoothed transverse electric field to
                             real space

Particle Push section:
   PPNCGUARD32L (cppncguard32l): fill in guard cells for smoothed
                                 longitudinal electric field in y and z
                                 from remote processor
   PPCGUARD32XL (cppcguard32xl): fill in guard cells for smoothed
                                 longitudinal electric field in x field
                                 on local processor
   PPNCGUARD32L (cppncguard32l): fill in guard cells for smoothed
                                 magnetic field in y and z from remote
                                 processor
   PPCGUARD32XL (cppcguard32xl): fill in guard cells for smoothed
                                 magnetic field in x field on local
                                 processor
   PPNCGUARD32L (cppncguard32l): fill in guard cells for smoothed
                                 transverse electric field in y and z
                                 from remote processor
   PPCGUARD32XL (cppcguard32xl): fill in guard cells for smoothed
                                 transverse electric field in x on local
                                 processor
   PPADDVRFIELD32 (cppaddvrfield32): add longitudinal and transverse
                                    electric fields
   PPGBPUSH32L (cppgbpush32l): update particle co-ordinates with
                               smoothed electric and magnetic fields
   PPMOVE32 (cppmove32): moves particles to appropriate processor from
                         from list supplied by PPGBPUSH32L
   PPDSORTP32YZL (cppdsortp32yzl) : sort particles by cell

The inputs to the code are the grid parameters indx, indy, indz, the
particle number parameters npx, npy, npz, the time parameters tend, dt,
the velocity parameters vtx, vty, vtz, vx0, vy0, vz0, the inverse speed
of light ci, magnetic field electron cyclotron frequencies omx, omy,
omz, the sorting parameter sortime, and number of corrections in darwin
iteration ndc

In more detail:
indx = exponent which determines length in x direction, nx=2**indx.
indy = exponent which determines length in y direction, ny=2**indy.
indz = exponent which determines length in z direction, nz=2**indz.
   These ensure the system lengths are a power of 2.
npx = number of electrons distributed in x direction.
npy = number of electrons distributed in y direction.
npz = number of electrons distributed in z direction.
   The total number of particles in the simulation is npx*npy*npz.
tend = time at end of simulation, in units of plasma frequency.
dt = time interval between successive calculations.
   The number of time steps the code runs is given by tend/dt.
   typically, dt should be less than .2 for the Darwin code
vtx/vty/vtz = thermal velocity of electrons in x/y/z direction.
   a typical value is 1.0.
vx0/vy0/vz0 = drift velocity of electrons in x/y/z direction.
ci = reciprocal of velocity of light
sortime = number of time steps between electron sorting.
   This is used to improve cache performance.  sortime=0 to suppress.
omx/omy/omz = magnetic field electron cyclotron frequency in x/y/z
ndc = number of corrections in darwin iteration
   typical values are 1 or 2.

The major program files contained here include:
pdpic3.f90    Fortran90 main program 
pdpic3.c      C main program
pplib3.f      Fortran77 MPI communications library
pplib3_h.f90  Fortran90 MPI communications interface (header) library
pplib3.f90    Fortran90 MPI communications library
pplib3.c      C MPI communications library [Not yet implemented]
pplib3.h      C MPI communications header library
pdpush3.f     Fortran77 procedure library
pdpush3_h.f90 Fortran90 procedure interface (header) library
pdpush3.c     C procedure library [Not yet implemented]
pdpush3.h     C procedure header library
dtimer.c      C timer function, used by both C and Fortran

Files with the suffix .f90 adhere to the Fortran 90 standard, files with
the suffix .f adhere to the Fortran77 standard, files with the suffix .c
and .h adhere to the C99 standard.

The makefile is setup to use gcc and gfortran with Linux.  A version for
Mac OS X is also in the Makefile, but is commented out.  

Two executables can be created, fpdpic3 for Fortran, and cpdpic3_f for a
C main program calling the Fortran procedure library.

To compile the programs, execute:

Make program_name

where program_name is either: fpdpic3 or cpdbpic3_f.

To execute, type the name of the executable:

mpirun -np nproc ./program_name

where program_name is either fpbpic3 or cpbpic3_f, and
where nproc is the number of processors to be used.

There is one restriction on the number of processors which can be used:
this simple skeleton code does not support the case where MPI nodes have
zero grid points.  This special case can happen for certain combinations
of the grid size in y or z (set by the parameters indy or indz,
respectively) and the number of processors in y and z.  If this happens
the code will exit with an error message.  This special case will never
occur if the grid size in y is an exact multiple of the number of
processors in y, and the grid size in z is an exact multiple of the
number of processors in z.

The file output contains the results produced for the default parameters.
Typical timing results are shown in the file fpdpic3_bench.pdf.

The Fortran version can be compiled to run with double precision by
changing the Makefile (typically by setting the compiler options flags
-r8).

The libraries pdpush3_f.c and pplib3_f.c contains wrapper functions to
allow the Fortran libraries to be called from C.
